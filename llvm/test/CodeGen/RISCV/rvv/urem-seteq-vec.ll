; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+m,+v < %s | FileCheck %s --check-prefix=RV32
; RUN: llc -mtriple=riscv64 -mattr=+m,+v < %s | FileCheck %s --check-prefix=RV64

define <vscale x 1 x i16> @test_urem_vec_even_divisor_eq0(<vscale x 1 x i16> %x) nounwind {
; RV32-LABEL: test_urem_vec_even_divisor_eq0:
; RV32:       # %bb.0:
; RV32-NEXT:    lui a0, 1048571
; RV32-NEXT:    addi a0, a0, -1365
; RV32-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV32-NEXT:    vmulhu.vx v9, v8, a0
; RV32-NEXT:    li a0, 6
; RV32-NEXT:    vsrl.vi v9, v9, 2
; RV32-NEXT:    vnmsub.vx v9, a0, v8
; RV32-NEXT:    vmsne.vi v0, v9, 0
; RV32-NEXT:    vmv.v.i v8, 0
; RV32-NEXT:    vmerge.vim v8, v8, -1, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: test_urem_vec_even_divisor_eq0:
; RV64:       # %bb.0:
; RV64-NEXT:    lui a0, 1048571
; RV64-NEXT:    addi a0, a0, -1365
; RV64-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV64-NEXT:    vmulhu.vx v9, v8, a0
; RV64-NEXT:    li a0, 6
; RV64-NEXT:    vsrl.vi v9, v9, 2
; RV64-NEXT:    vnmsub.vx v9, a0, v8
; RV64-NEXT:    vmsne.vi v0, v9, 0
; RV64-NEXT:    vmv.v.i v8, 0
; RV64-NEXT:    vmerge.vim v8, v8, -1, v0
; RV64-NEXT:    ret
  %urem = urem <vscale x 1 x i16> %x, splat (i16 6)
  %cmp = icmp ne <vscale x 1 x i16> %urem, splat (i16 0)
  %ext = sext <vscale x 1 x i1> %cmp to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %ext
}

define <vscale x 1 x i16> @test_urem_vec_odd_divisor_eq0(<vscale x 1 x i16> %x) nounwind {
; RV32-LABEL: test_urem_vec_odd_divisor_eq0:
; RV32:       # %bb.0:
; RV32-NEXT:    lui a0, 1048573
; RV32-NEXT:    addi a0, a0, -819
; RV32-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV32-NEXT:    vmulhu.vx v9, v8, a0
; RV32-NEXT:    li a0, 5
; RV32-NEXT:    vsrl.vi v9, v9, 2
; RV32-NEXT:    vnmsub.vx v9, a0, v8
; RV32-NEXT:    vmsne.vi v0, v9, 0
; RV32-NEXT:    vmv.v.i v8, 0
; RV32-NEXT:    vmerge.vim v8, v8, -1, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: test_urem_vec_odd_divisor_eq0:
; RV64:       # %bb.0:
; RV64-NEXT:    lui a0, 1048573
; RV64-NEXT:    addi a0, a0, -819
; RV64-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV64-NEXT:    vmulhu.vx v9, v8, a0
; RV64-NEXT:    li a0, 5
; RV64-NEXT:    vsrl.vi v9, v9, 2
; RV64-NEXT:    vnmsub.vx v9, a0, v8
; RV64-NEXT:    vmsne.vi v0, v9, 0
; RV64-NEXT:    vmv.v.i v8, 0
; RV64-NEXT:    vmerge.vim v8, v8, -1, v0
; RV64-NEXT:    ret
  %urem = urem <vscale x 1 x i16> %x, splat (i16 5)
  %cmp = icmp ne <vscale x 1 x i16> %urem, splat (i16 0)
  %ext = sext <vscale x 1 x i1> %cmp to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %ext
}

define <vscale x 1 x i16> @test_urem_vec_even_divisor_eq1(<vscale x 1 x i16> %x) nounwind {
; RV32-LABEL: test_urem_vec_even_divisor_eq1:
; RV32:       # %bb.0:
; RV32-NEXT:    lui a0, 1048571
; RV32-NEXT:    addi a0, a0, -1365
; RV32-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV32-NEXT:    vmulhu.vx v9, v8, a0
; RV32-NEXT:    li a0, 6
; RV32-NEXT:    vsrl.vi v9, v9, 2
; RV32-NEXT:    vnmsub.vx v9, a0, v8
; RV32-NEXT:    vmsne.vi v0, v9, 1
; RV32-NEXT:    vmv.v.i v8, 0
; RV32-NEXT:    vmerge.vim v8, v8, -1, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: test_urem_vec_even_divisor_eq1:
; RV64:       # %bb.0:
; RV64-NEXT:    lui a0, 1048571
; RV64-NEXT:    addi a0, a0, -1365
; RV64-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV64-NEXT:    vmulhu.vx v9, v8, a0
; RV64-NEXT:    li a0, 6
; RV64-NEXT:    vsrl.vi v9, v9, 2
; RV64-NEXT:    vnmsub.vx v9, a0, v8
; RV64-NEXT:    vmsne.vi v0, v9, 1
; RV64-NEXT:    vmv.v.i v8, 0
; RV64-NEXT:    vmerge.vim v8, v8, -1, v0
; RV64-NEXT:    ret
  %urem = urem <vscale x 1 x i16> %x, splat (i16 6)
  %cmp = icmp ne <vscale x 1 x i16> %urem, splat (i16 1)
  %ext = sext <vscale x 1 x i1> %cmp to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %ext
}

define <vscale x 1 x i16> @test_urem_vec_odd_divisor_eq1(<vscale x 1 x i16> %x) nounwind {
; RV32-LABEL: test_urem_vec_odd_divisor_eq1:
; RV32:       # %bb.0:
; RV32-NEXT:    lui a0, 1048573
; RV32-NEXT:    addi a0, a0, -819
; RV32-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV32-NEXT:    vmulhu.vx v9, v8, a0
; RV32-NEXT:    li a0, 5
; RV32-NEXT:    vsrl.vi v9, v9, 2
; RV32-NEXT:    vnmsub.vx v9, a0, v8
; RV32-NEXT:    vmsne.vi v0, v9, 1
; RV32-NEXT:    vmv.v.i v8, 0
; RV32-NEXT:    vmerge.vim v8, v8, -1, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: test_urem_vec_odd_divisor_eq1:
; RV64:       # %bb.0:
; RV64-NEXT:    lui a0, 1048573
; RV64-NEXT:    addi a0, a0, -819
; RV64-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; RV64-NEXT:    vmulhu.vx v9, v8, a0
; RV64-NEXT:    li a0, 5
; RV64-NEXT:    vsrl.vi v9, v9, 2
; RV64-NEXT:    vnmsub.vx v9, a0, v8
; RV64-NEXT:    vmsne.vi v0, v9, 1
; RV64-NEXT:    vmv.v.i v8, 0
; RV64-NEXT:    vmerge.vim v8, v8, -1, v0
; RV64-NEXT:    ret
  %urem = urem <vscale x 1 x i16> %x, splat (i16 5)
  %cmp = icmp ne <vscale x 1 x i16> %urem, splat (i16 1)
  %ext = sext <vscale x 1 x i1> %cmp to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %ext
}
