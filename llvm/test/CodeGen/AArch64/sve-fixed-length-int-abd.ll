; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -aarch64-sve-vector-bits-min=256  < %s | FileCheck %s -check-prefixes=CHECK,VBITS_GE_256
; RUN: llc -aarch64-sve-vector-bits-min=512  < %s | FileCheck %s -check-prefixes=CHECK,VBITS_GE_512,VBITS_EQ_512
; RUN: llc -aarch64-sve-vector-bits-min=2048 < %s | FileCheck %s -check-prefixes=CHECK,VBITS_GE_512,VBITS_GE_2048

target triple = "aarch64-unknown-linux-gnu"

; Don't use SVE for 128-bit vectors.
define void @sabd_v16i8_v16i16(ptr %a, ptr %b) #0 {
; CHECK-LABEL: sabd_v16i8_v16i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.h, vl16
; CHECK-NEXT:    ld1sb { z0.h }, p0/z, [x0]
; CHECK-NEXT:    ld1sb { z1.h }, p0/z, [x1]
; CHECK-NEXT:    sabd z0.h, p0/m, z0.h, z1.h
; CHECK-NEXT:    st1b { z0.h }, p0, [x0]
; CHECK-NEXT:    ret
  %a.ld = load <16 x i8>, ptr %a
  %b.ld = load <16 x i8>, ptr %b
  %a.sext = sext <16 x i8> %a.ld to <16 x i16>
  %b.sext = sext <16 x i8> %b.ld to <16 x i16>
  %sub = sub <16 x i16> %a.sext, %b.sext
  %abs = call <16 x i16> @llvm.abs.v16i16(<16 x i16> %sub, i1 true)
  %trunc = trunc <16 x i16> %abs to <16 x i8>
  store <16 x i8> %trunc, ptr %a
  ret void
}

; Don't use SVE for 128-bit vectors.
define void @sabd_v16i8_v16i32(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v16i8_v16i32:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.s, vl8
; VBITS_GE_256-NEXT:    mov w8, #8 // =0x8
; VBITS_GE_256-NEXT:    ld1sb { z0.s }, p0/z, [x0, x8]
; VBITS_GE_256-NEXT:    ld1sb { z1.s }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1sb { z2.s }, p0/z, [x1, x8]
; VBITS_GE_256-NEXT:    ld1sb { z3.s }, p0/z, [x1]
; VBITS_GE_256-NEXT:    sabd z0.s, p0/m, z0.s, z2.s
; VBITS_GE_256-NEXT:    sabd z1.s, p0/m, z1.s, z3.s
; VBITS_GE_256-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_GE_256-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_GE_256-NEXT:    str q1, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_GE_512-LABEL: sabd_v16i8_v16i32:
; VBITS_GE_512:       // %bb.0:
; VBITS_GE_512-NEXT:    ptrue p0.s, vl16
; VBITS_GE_512-NEXT:    ld1sb { z0.s }, p0/z, [x0]
; VBITS_GE_512-NEXT:    ld1sb { z1.s }, p0/z, [x1]
; VBITS_GE_512-NEXT:    sabd z0.s, p0/m, z0.s, z1.s
; VBITS_GE_512-NEXT:    st1b { z0.s }, p0, [x0]
; VBITS_GE_512-NEXT:    ret
  %a.ld = load <16 x i8>, ptr %a
  %b.ld = load <16 x i8>, ptr %b
  %a.sext = sext <16 x i8> %a.ld to <16 x i32>
  %b.sext = sext <16 x i8> %b.ld to <16 x i32>
  %sub = sub <16 x i32> %a.sext, %b.sext
  %abs = call <16 x i32> @llvm.abs.v16i32(<16 x i32> %sub, i1 true)
  %trunc = trunc <16 x i32> %abs to <16 x i8>
  store <16 x i8> %trunc, ptr %a
  ret void
}

; Don't use SVE for 128-bit vectors.
define void @sabd_v16i8_v16i64(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v16i8_v16i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.d, vl4
; VBITS_GE_256-NEXT:    mov w8, #12 // =0xc
; VBITS_GE_256-NEXT:    mov w9, #8 // =0x8
; VBITS_GE_256-NEXT:    mov w10, #4 // =0x4
; VBITS_GE_256-NEXT:    ld1sb { z0.d }, p0/z, [x0, x8]
; VBITS_GE_256-NEXT:    ld1sb { z1.d }, p0/z, [x0, x9]
; VBITS_GE_256-NEXT:    ld1sb { z2.d }, p0/z, [x0, x10]
; VBITS_GE_256-NEXT:    ld1sb { z3.d }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1sb { z4.d }, p0/z, [x1, x8]
; VBITS_GE_256-NEXT:    ld1sb { z5.d }, p0/z, [x1, x9]
; VBITS_GE_256-NEXT:    ld1sb { z6.d }, p0/z, [x1, x10]
; VBITS_GE_256-NEXT:    ld1sb { z7.d }, p0/z, [x1]
; VBITS_GE_256-NEXT:    sabd z0.d, p0/m, z0.d, z4.d
; VBITS_GE_256-NEXT:    sabd z1.d, p0/m, z1.d, z5.d
; VBITS_GE_256-NEXT:    sabd z2.d, p0/m, z2.d, z6.d
; VBITS_GE_256-NEXT:    sabd z3.d, p0/m, z3.d, z7.d
; VBITS_GE_256-NEXT:    ptrue p0.s, vl4
; VBITS_GE_256-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_GE_256-NEXT:    uzp1 z1.s, z1.s, z1.s
; VBITS_GE_256-NEXT:    uzp1 z2.s, z2.s, z2.s
; VBITS_GE_256-NEXT:    uzp1 z3.s, z3.s, z3.s
; VBITS_GE_256-NEXT:    splice z1.s, p0, z1.s, z0.s
; VBITS_GE_256-NEXT:    splice z3.s, p0, z3.s, z2.s
; VBITS_GE_256-NEXT:    uzp1 z0.h, z1.h, z1.h
; VBITS_GE_256-NEXT:    uzp1 z1.h, z3.h, z3.h
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_GE_256-NEXT:    str q1, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v16i8_v16i64:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ptrue p0.d, vl8
; VBITS_EQ_512-NEXT:    mov w8, #8 // =0x8
; VBITS_EQ_512-NEXT:    ld1sb { z0.d }, p0/z, [x0, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z1.d }, p0/z, [x0]
; VBITS_EQ_512-NEXT:    ld1sb { z2.d }, p0/z, [x1, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z3.d }, p0/z, [x1]
; VBITS_EQ_512-NEXT:    sabd z0.d, p0/m, z0.d, z2.d
; VBITS_EQ_512-NEXT:    sabd z1.d, p0/m, z1.d, z3.d
; VBITS_EQ_512-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_EQ_512-NEXT:    uzp1 z1.s, z1.s, z1.s
; VBITS_EQ_512-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_EQ_512-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_EQ_512-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_EQ_512-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_EQ_512-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_EQ_512-NEXT:    str q1, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v16i8_v16i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.d, vl16
; VBITS_GE_2048-NEXT:    ld1sb { z0.d }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1sb { z1.d }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    sabd z0.d, p0/m, z0.d, z1.d
; VBITS_GE_2048-NEXT:    st1b { z0.d }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <16 x i8>, ptr %a
  %b.ld = load <16 x i8>, ptr %b
  %a.sext = sext <16 x i8> %a.ld to <16 x i64>
  %b.sext = sext <16 x i8> %b.ld to <16 x i64>
  %sub = sub <16 x i64> %a.sext, %b.sext
  %abs = call <16 x i64> @llvm.abs.v16i64(<16 x i64> %sub, i1 true)
  %trunc = trunc <16 x i64> %abs to <16 x i8>
  store <16 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v32i8_v32i16(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v32i8_v32i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.h, vl16
; VBITS_GE_256-NEXT:    mov w8, #16 // =0x10
; VBITS_GE_256-NEXT:    ld1sb { z0.h }, p0/z, [x0, x8]
; VBITS_GE_256-NEXT:    ld1sb { z1.h }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1sb { z2.h }, p0/z, [x1, x8]
; VBITS_GE_256-NEXT:    ld1sb { z3.h }, p0/z, [x1]
; VBITS_GE_256-NEXT:    sabd z0.h, p0/m, z0.h, z2.h
; VBITS_GE_256-NEXT:    sabd z1.h, p0/m, z1.h, z3.h
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    splice z1.b, p0, z1.b, z0.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z1.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_GE_512-LABEL: sabd_v32i8_v32i16:
; VBITS_GE_512:       // %bb.0:
; VBITS_GE_512-NEXT:    ptrue p0.h, vl32
; VBITS_GE_512-NEXT:    ld1sb { z0.h }, p0/z, [x0]
; VBITS_GE_512-NEXT:    ld1sb { z1.h }, p0/z, [x1]
; VBITS_GE_512-NEXT:    sabd z0.h, p0/m, z0.h, z1.h
; VBITS_GE_512-NEXT:    st1b { z0.h }, p0, [x0]
; VBITS_GE_512-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.sext = sext <32 x i8> %a.ld to <32 x i16>
  %b.sext = sext <32 x i8> %b.ld to <32 x i16>
  %sub = sub <32 x i16> %a.sext, %b.sext
  %abs = call <32 x i16> @llvm.abs.v32i16(<32 x i16> %sub, i1 true)
  %trunc = trunc <32 x i16> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @uabd_v32i8_v32i16(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: uabd_v32i8_v32i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.h, vl16
; VBITS_GE_256-NEXT:    mov w8, #16 // =0x10
; VBITS_GE_256-NEXT:    ld1b { z0.h }, p0/z, [x0, x8]
; VBITS_GE_256-NEXT:    ld1b { z1.h }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1b { z2.h }, p0/z, [x1, x8]
; VBITS_GE_256-NEXT:    ld1b { z3.h }, p0/z, [x1]
; VBITS_GE_256-NEXT:    uabd z0.h, p0/m, z0.h, z2.h
; VBITS_GE_256-NEXT:    uabd z1.h, p0/m, z1.h, z3.h
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    splice z1.b, p0, z1.b, z0.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z1.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_GE_512-LABEL: uabd_v32i8_v32i16:
; VBITS_GE_512:       // %bb.0:
; VBITS_GE_512-NEXT:    ptrue p0.h, vl32
; VBITS_GE_512-NEXT:    ld1b { z0.h }, p0/z, [x0]
; VBITS_GE_512-NEXT:    ld1b { z1.h }, p0/z, [x1]
; VBITS_GE_512-NEXT:    uabd z0.h, p0/m, z0.h, z1.h
; VBITS_GE_512-NEXT:    st1b { z0.h }, p0, [x0]
; VBITS_GE_512-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.zext = zext <32 x i8> %a.ld to <32 x i16>
  %b.zext = zext <32 x i8> %b.ld to <32 x i16>
  %sub = sub <32 x i16> %a.zext, %b.zext
  %abs = call <32 x i16> @llvm.abs.v32i16(<32 x i16> %sub, i1 true)
  %trunc = trunc <32 x i16> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v32i8_v32i32(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v32i8_v32i32:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.s, vl8
; VBITS_GE_256-NEXT:    mov w8, #24 // =0x18
; VBITS_GE_256-NEXT:    mov w9, #16 // =0x10
; VBITS_GE_256-NEXT:    mov w10, #8 // =0x8
; VBITS_GE_256-NEXT:    ld1sb { z0.s }, p0/z, [x0, x8]
; VBITS_GE_256-NEXT:    ld1sb { z1.s }, p0/z, [x0, x9]
; VBITS_GE_256-NEXT:    ld1sb { z2.s }, p0/z, [x0, x10]
; VBITS_GE_256-NEXT:    ld1sb { z3.s }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1sb { z4.s }, p0/z, [x1, x8]
; VBITS_GE_256-NEXT:    ld1sb { z5.s }, p0/z, [x1, x9]
; VBITS_GE_256-NEXT:    ld1sb { z6.s }, p0/z, [x1, x10]
; VBITS_GE_256-NEXT:    ld1sb { z7.s }, p0/z, [x1]
; VBITS_GE_256-NEXT:    sabd z0.s, p0/m, z0.s, z4.s
; VBITS_GE_256-NEXT:    sabd z1.s, p0/m, z1.s, z5.s
; VBITS_GE_256-NEXT:    sabd z2.s, p0/m, z2.s, z6.s
; VBITS_GE_256-NEXT:    sabd z3.s, p0/m, z3.s, z7.s
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_GE_256-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_GE_256-NEXT:    uzp1 z2.h, z2.h, z2.h
; VBITS_GE_256-NEXT:    uzp1 z3.h, z3.h, z3.h
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    uzp1 z2.b, z2.b, z2.b
; VBITS_GE_256-NEXT:    uzp1 z3.b, z3.b, z3.b
; VBITS_GE_256-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_GE_256-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_GE_256-NEXT:    splice z3.b, p0, z3.b, z1.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z3.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v32i8_v32i32:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ptrue p0.s, vl16
; VBITS_EQ_512-NEXT:    mov w8, #16 // =0x10
; VBITS_EQ_512-NEXT:    ld1sb { z0.s }, p0/z, [x0, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z1.s }, p0/z, [x0]
; VBITS_EQ_512-NEXT:    ld1sb { z2.s }, p0/z, [x1, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z3.s }, p0/z, [x1]
; VBITS_EQ_512-NEXT:    sabd z0.s, p0/m, z0.s, z2.s
; VBITS_EQ_512-NEXT:    sabd z1.s, p0/m, z1.s, z3.s
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl16
; VBITS_EQ_512-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_EQ_512-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_EQ_512-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_EQ_512-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_EQ_512-NEXT:    splice z1.b, p0, z1.b, z0.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl32
; VBITS_EQ_512-NEXT:    st1b { z1.b }, p0, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v32i8_v32i32:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.s, vl32
; VBITS_GE_2048-NEXT:    ld1sb { z0.s }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1sb { z1.s }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    sabd z0.s, p0/m, z0.s, z1.s
; VBITS_GE_2048-NEXT:    st1b { z0.s }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.sext = sext <32 x i8> %a.ld to <32 x i32>
  %b.sext = sext <32 x i8> %b.ld to <32 x i32>
  %sub = sub <32 x i32> %a.sext, %b.sext
  %abs = call <32 x i32> @llvm.abs.v32i32(<32 x i32> %sub, i1 true)
  %trunc = trunc <32 x i32> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v32i8_v32i64(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v32i8_v32i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.d, vl4
; VBITS_GE_256-NEXT:    mov w8, #28 // =0x1c
; VBITS_GE_256-NEXT:    mov w9, #24 // =0x18
; VBITS_GE_256-NEXT:    mov w10, #20 // =0x14
; VBITS_GE_256-NEXT:    mov w11, #16 // =0x10
; VBITS_GE_256-NEXT:    mov w12, #12 // =0xc
; VBITS_GE_256-NEXT:    mov w13, #8 // =0x8
; VBITS_GE_256-NEXT:    mov w14, #4 // =0x4
; VBITS_GE_256-NEXT:    ld1sb { z0.d }, p0/z, [x0, x8]
; VBITS_GE_256-NEXT:    ld1sb { z1.d }, p0/z, [x0, x9]
; VBITS_GE_256-NEXT:    ld1sb { z2.d }, p0/z, [x0, x10]
; VBITS_GE_256-NEXT:    ld1sb { z3.d }, p0/z, [x0, x11]
; VBITS_GE_256-NEXT:    ld1sb { z4.d }, p0/z, [x0, x12]
; VBITS_GE_256-NEXT:    ld1sb { z5.d }, p0/z, [x0, x13]
; VBITS_GE_256-NEXT:    ld1sb { z6.d }, p0/z, [x0, x14]
; VBITS_GE_256-NEXT:    ld1sb { z7.d }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1sb { z16.d }, p0/z, [x1, x8]
; VBITS_GE_256-NEXT:    ld1sb { z17.d }, p0/z, [x1, x9]
; VBITS_GE_256-NEXT:    ld1sb { z18.d }, p0/z, [x1, x10]
; VBITS_GE_256-NEXT:    ld1sb { z19.d }, p0/z, [x1, x11]
; VBITS_GE_256-NEXT:    ld1sb { z20.d }, p0/z, [x1, x12]
; VBITS_GE_256-NEXT:    ld1sb { z21.d }, p0/z, [x1, x13]
; VBITS_GE_256-NEXT:    ld1sb { z22.d }, p0/z, [x1, x14]
; VBITS_GE_256-NEXT:    ld1sb { z23.d }, p0/z, [x1]
; VBITS_GE_256-NEXT:    sabd z1.d, p0/m, z1.d, z17.d
; VBITS_GE_256-NEXT:    sabd z0.d, p0/m, z0.d, z16.d
; VBITS_GE_256-NEXT:    sabd z4.d, p0/m, z4.d, z20.d
; VBITS_GE_256-NEXT:    sabd z3.d, p0/m, z3.d, z19.d
; VBITS_GE_256-NEXT:    sabd z2.d, p0/m, z2.d, z18.d
; VBITS_GE_256-NEXT:    sabd z6.d, p0/m, z6.d, z22.d
; VBITS_GE_256-NEXT:    sabd z5.d, p0/m, z5.d, z21.d
; VBITS_GE_256-NEXT:    sabd z7.d, p0/m, z7.d, z23.d
; VBITS_GE_256-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_GE_256-NEXT:    uzp1 z1.s, z1.s, z1.s
; VBITS_GE_256-NEXT:    ptrue p0.s, vl4
; VBITS_GE_256-NEXT:    uzp1 z2.s, z2.s, z2.s
; VBITS_GE_256-NEXT:    uzp1 z3.s, z3.s, z3.s
; VBITS_GE_256-NEXT:    uzp1 z4.s, z4.s, z4.s
; VBITS_GE_256-NEXT:    uzp1 z5.s, z5.s, z5.s
; VBITS_GE_256-NEXT:    uzp1 z6.s, z6.s, z6.s
; VBITS_GE_256-NEXT:    uzp1 z7.s, z7.s, z7.s
; VBITS_GE_256-NEXT:    splice z1.s, p0, z1.s, z0.s
; VBITS_GE_256-NEXT:    splice z3.s, p0, z3.s, z2.s
; VBITS_GE_256-NEXT:    splice z5.s, p0, z5.s, z4.s
; VBITS_GE_256-NEXT:    splice z7.s, p0, z7.s, z6.s
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    uzp1 z0.h, z1.h, z1.h
; VBITS_GE_256-NEXT:    uzp1 z1.h, z3.h, z3.h
; VBITS_GE_256-NEXT:    uzp1 z2.h, z5.h, z5.h
; VBITS_GE_256-NEXT:    uzp1 z3.h, z7.h, z7.h
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    uzp1 z2.b, z2.b, z2.b
; VBITS_GE_256-NEXT:    uzp1 z3.b, z3.b, z3.b
; VBITS_GE_256-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_GE_256-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_GE_256-NEXT:    splice z3.b, p0, z3.b, z1.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z3.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v32i8_v32i64:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ptrue p0.d, vl8
; VBITS_EQ_512-NEXT:    mov w8, #24 // =0x18
; VBITS_EQ_512-NEXT:    mov w9, #16 // =0x10
; VBITS_EQ_512-NEXT:    mov w10, #8 // =0x8
; VBITS_EQ_512-NEXT:    ld1sb { z0.d }, p0/z, [x0, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z1.d }, p0/z, [x0, x9]
; VBITS_EQ_512-NEXT:    ld1sb { z2.d }, p0/z, [x0, x10]
; VBITS_EQ_512-NEXT:    ld1sb { z3.d }, p0/z, [x0]
; VBITS_EQ_512-NEXT:    ld1sb { z4.d }, p0/z, [x1, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z5.d }, p0/z, [x1, x9]
; VBITS_EQ_512-NEXT:    ld1sb { z6.d }, p0/z, [x1, x10]
; VBITS_EQ_512-NEXT:    ld1sb { z7.d }, p0/z, [x1]
; VBITS_EQ_512-NEXT:    sabd z0.d, p0/m, z0.d, z4.d
; VBITS_EQ_512-NEXT:    sabd z1.d, p0/m, z1.d, z5.d
; VBITS_EQ_512-NEXT:    sabd z2.d, p0/m, z2.d, z6.d
; VBITS_EQ_512-NEXT:    sabd z3.d, p0/m, z3.d, z7.d
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl16
; VBITS_EQ_512-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_EQ_512-NEXT:    uzp1 z1.s, z1.s, z1.s
; VBITS_EQ_512-NEXT:    uzp1 z2.s, z2.s, z2.s
; VBITS_EQ_512-NEXT:    uzp1 z3.s, z3.s, z3.s
; VBITS_EQ_512-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_EQ_512-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_EQ_512-NEXT:    uzp1 z2.h, z2.h, z2.h
; VBITS_EQ_512-NEXT:    uzp1 z3.h, z3.h, z3.h
; VBITS_EQ_512-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_EQ_512-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_EQ_512-NEXT:    uzp1 z2.b, z2.b, z2.b
; VBITS_EQ_512-NEXT:    uzp1 z3.b, z3.b, z3.b
; VBITS_EQ_512-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_EQ_512-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_EQ_512-NEXT:    splice z3.b, p0, z3.b, z1.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl32
; VBITS_EQ_512-NEXT:    st1b { z3.b }, p0, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v32i8_v32i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.d, vl32
; VBITS_GE_2048-NEXT:    ld1sb { z0.d }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1sb { z1.d }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    sabd z0.d, p0/m, z0.d, z1.d
; VBITS_GE_2048-NEXT:    st1b { z0.d }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <32 x i8>, ptr %a
  %b.ld = load <32 x i8>, ptr %b
  %a.sext = sext <32 x i8> %a.ld to <32 x i64>
  %b.sext = sext <32 x i8> %b.ld to <32 x i64>
  %sub = sub <32 x i64> %a.sext, %b.sext
  %abs = call <32 x i64> @llvm.abs.v32i64(<32 x i64> %sub, i1 true)
  %trunc = trunc <32 x i64> %abs to <32 x i8>
  store <32 x i8> %trunc, ptr %a
  ret void
}

define void @sabd_v64i8_v64i64(ptr %a, ptr %b) #0 {
; VBITS_GE_256-LABEL: sabd_v64i8_v64i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    str d14, [sp, #-64]! // 8-byte Folded Spill
; VBITS_GE_256-NEXT:    stp d13, d12, [sp, #16] // 16-byte Folded Spill
; VBITS_GE_256-NEXT:    stp d11, d10, [sp, #32] // 16-byte Folded Spill
; VBITS_GE_256-NEXT:    stp d9, d8, [sp, #48] // 16-byte Folded Spill
; VBITS_GE_256-NEXT:    .cfi_def_cfa_offset 64
; VBITS_GE_256-NEXT:    .cfi_offset b8, -8
; VBITS_GE_256-NEXT:    .cfi_offset b9, -16
; VBITS_GE_256-NEXT:    .cfi_offset b10, -24
; VBITS_GE_256-NEXT:    .cfi_offset b11, -32
; VBITS_GE_256-NEXT:    .cfi_offset b12, -40
; VBITS_GE_256-NEXT:    .cfi_offset b13, -48
; VBITS_GE_256-NEXT:    .cfi_offset b14, -64
; VBITS_GE_256-NEXT:    ptrue p0.d, vl4
; VBITS_GE_256-NEXT:    mov w5, #4 // =0x4
; VBITS_GE_256-NEXT:    mov w9, #60 // =0x3c
; VBITS_GE_256-NEXT:    mov w10, #56 // =0x38
; VBITS_GE_256-NEXT:    mov w11, #52 // =0x34
; VBITS_GE_256-NEXT:    mov w12, #48 // =0x30
; VBITS_GE_256-NEXT:    ld1sb { z0.d }, p0/z, [x0, x5]
; VBITS_GE_256-NEXT:    ld1sb { z30.d }, p0/z, [x1, x5]
; VBITS_GE_256-NEXT:    mov w13, #44 // =0x2c
; VBITS_GE_256-NEXT:    mov w14, #40 // =0x28
; VBITS_GE_256-NEXT:    mov w15, #36 // =0x24
; VBITS_GE_256-NEXT:    mov w8, #32 // =0x20
; VBITS_GE_256-NEXT:    mov w16, #28 // =0x1c
; VBITS_GE_256-NEXT:    mov w17, #24 // =0x18
; VBITS_GE_256-NEXT:    mov w18, #20 // =0x14
; VBITS_GE_256-NEXT:    mov w2, #16 // =0x10
; VBITS_GE_256-NEXT:    mov w3, #12 // =0xc
; VBITS_GE_256-NEXT:    mov w4, #8 // =0x8
; VBITS_GE_256-NEXT:    ld1sb { z1.d }, p0/z, [x0, x9]
; VBITS_GE_256-NEXT:    ld1sb { z3.d }, p0/z, [x0, x10]
; VBITS_GE_256-NEXT:    ld1sb { z2.d }, p0/z, [x0, x11]
; VBITS_GE_256-NEXT:    ld1sb { z5.d }, p0/z, [x0, x12]
; VBITS_GE_256-NEXT:    ld1sb { z4.d }, p0/z, [x0, x13]
; VBITS_GE_256-NEXT:    ld1sb { z7.d }, p0/z, [x0, x14]
; VBITS_GE_256-NEXT:    ld1sb { z17.d }, p0/z, [x0, x15]
; VBITS_GE_256-NEXT:    ld1sb { z18.d }, p0/z, [x0, x8]
; VBITS_GE_256-NEXT:    ld1sb { z20.d }, p0/z, [x0, x16]
; VBITS_GE_256-NEXT:    ld1sb { z21.d }, p0/z, [x0, x17]
; VBITS_GE_256-NEXT:    ld1sb { z22.d }, p0/z, [x0, x18]
; VBITS_GE_256-NEXT:    ld1sb { z19.d }, p0/z, [x0, x2]
; VBITS_GE_256-NEXT:    ld1sb { z16.d }, p0/z, [x0, x3]
; VBITS_GE_256-NEXT:    ld1sb { z6.d }, p0/z, [x0, x4]
; VBITS_GE_256-NEXT:    ld1sb { z23.d }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1sb { z24.d }, p0/z, [x1, x9]
; VBITS_GE_256-NEXT:    ld1sb { z25.d }, p0/z, [x1, x10]
; VBITS_GE_256-NEXT:    ld1sb { z26.d }, p0/z, [x1, x11]
; VBITS_GE_256-NEXT:    ld1sb { z27.d }, p0/z, [x1, x12]
; VBITS_GE_256-NEXT:    ld1sb { z28.d }, p0/z, [x1, x13]
; VBITS_GE_256-NEXT:    ld1sb { z29.d }, p0/z, [x1, x14]
; VBITS_GE_256-NEXT:    ld1sb { z31.d }, p0/z, [x1, x15]
; VBITS_GE_256-NEXT:    ld1sb { z8.d }, p0/z, [x1, x3]
; VBITS_GE_256-NEXT:    ld1sb { z9.d }, p0/z, [x1, x4]
; VBITS_GE_256-NEXT:    ld1sb { z10.d }, p0/z, [x1, x17]
; VBITS_GE_256-NEXT:    ld1sb { z11.d }, p0/z, [x1, x18]
; VBITS_GE_256-NEXT:    ld1sb { z12.d }, p0/z, [x1, x2]
; VBITS_GE_256-NEXT:    ld1sb { z13.d }, p0/z, [x1, x8]
; VBITS_GE_256-NEXT:    ld1sb { z14.d }, p0/z, [x1, x16]
; VBITS_GE_256-NEXT:    sabd z0.d, p0/m, z0.d, z30.d
; VBITS_GE_256-NEXT:    ld1sb { z30.d }, p0/z, [x1]
; VBITS_GE_256-NEXT:    sabd z6.d, p0/m, z6.d, z9.d
; VBITS_GE_256-NEXT:    sabd z16.d, p0/m, z16.d, z8.d
; VBITS_GE_256-NEXT:    sabd z19.d, p0/m, z19.d, z12.d
; VBITS_GE_256-NEXT:    sabd z22.d, p0/m, z22.d, z11.d
; VBITS_GE_256-NEXT:    sabd z21.d, p0/m, z21.d, z10.d
; VBITS_GE_256-NEXT:    sabd z20.d, p0/m, z20.d, z14.d
; VBITS_GE_256-NEXT:    sabd z3.d, p0/m, z3.d, z25.d
; VBITS_GE_256-NEXT:    sabd z1.d, p0/m, z1.d, z24.d
; VBITS_GE_256-NEXT:    sabd z18.d, p0/m, z18.d, z13.d
; VBITS_GE_256-NEXT:    sabd z5.d, p0/m, z5.d, z27.d
; VBITS_GE_256-NEXT:    sabd z2.d, p0/m, z2.d, z26.d
; VBITS_GE_256-NEXT:    sabd z17.d, p0/m, z17.d, z31.d
; VBITS_GE_256-NEXT:    sabd z7.d, p0/m, z7.d, z29.d
; VBITS_GE_256-NEXT:    sabd z4.d, p0/m, z4.d, z28.d
; VBITS_GE_256-NEXT:    sabd z23.d, p0/m, z23.d, z30.d
; VBITS_GE_256-NEXT:    uzp1 z24.s, z1.s, z1.s
; VBITS_GE_256-NEXT:    uzp1 z1.s, z3.s, z3.s
; VBITS_GE_256-NEXT:    uzp1 z3.s, z2.s, z2.s
; VBITS_GE_256-NEXT:    uzp1 z2.s, z5.s, z5.s
; VBITS_GE_256-NEXT:    uzp1 z19.s, z19.s, z19.s
; VBITS_GE_256-NEXT:    uzp1 z4.s, z4.s, z4.s
; VBITS_GE_256-NEXT:    uzp1 z5.s, z7.s, z7.s
; VBITS_GE_256-NEXT:    uzp1 z7.s, z17.s, z17.s
; VBITS_GE_256-NEXT:    uzp1 z17.s, z18.s, z18.s
; VBITS_GE_256-NEXT:    uzp1 z18.s, z20.s, z20.s
; VBITS_GE_256-NEXT:    uzp1 z20.s, z21.s, z21.s
; VBITS_GE_256-NEXT:    uzp1 z21.s, z22.s, z22.s
; VBITS_GE_256-NEXT:    uzp1 z16.s, z16.s, z16.s
; VBITS_GE_256-NEXT:    uzp1 z6.s, z6.s, z6.s
; VBITS_GE_256-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_GE_256-NEXT:    uzp1 z22.s, z23.s, z23.s
; VBITS_GE_256-NEXT:    ptrue p0.s, vl4
; VBITS_GE_256-NEXT:    ldp d9, d8, [sp, #48] // 16-byte Folded Reload
; VBITS_GE_256-NEXT:    splice z1.s, p0, z1.s, z24.s
; VBITS_GE_256-NEXT:    splice z2.s, p0, z2.s, z3.s
; VBITS_GE_256-NEXT:    splice z5.s, p0, z5.s, z4.s
; VBITS_GE_256-NEXT:    splice z17.s, p0, z17.s, z7.s
; VBITS_GE_256-NEXT:    splice z20.s, p0, z20.s, z18.s
; VBITS_GE_256-NEXT:    splice z19.s, p0, z19.s, z21.s
; VBITS_GE_256-NEXT:    splice z6.s, p0, z6.s, z16.s
; VBITS_GE_256-NEXT:    splice z22.s, p0, z22.s, z0.s
; VBITS_GE_256-NEXT:    ptrue p0.b, vl16
; VBITS_GE_256-NEXT:    ldp d11, d10, [sp, #32] // 16-byte Folded Reload
; VBITS_GE_256-NEXT:    uzp1 z0.h, z1.h, z1.h
; VBITS_GE_256-NEXT:    uzp1 z1.h, z2.h, z2.h
; VBITS_GE_256-NEXT:    uzp1 z2.h, z5.h, z5.h
; VBITS_GE_256-NEXT:    uzp1 z3.h, z17.h, z17.h
; VBITS_GE_256-NEXT:    uzp1 z4.h, z20.h, z20.h
; VBITS_GE_256-NEXT:    uzp1 z5.h, z19.h, z19.h
; VBITS_GE_256-NEXT:    uzp1 z6.h, z6.h, z6.h
; VBITS_GE_256-NEXT:    uzp1 z7.h, z22.h, z22.h
; VBITS_GE_256-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_256-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_256-NEXT:    uzp1 z2.b, z2.b, z2.b
; VBITS_GE_256-NEXT:    uzp1 z3.b, z3.b, z3.b
; VBITS_GE_256-NEXT:    uzp1 z4.b, z4.b, z4.b
; VBITS_GE_256-NEXT:    uzp1 z5.b, z5.b, z5.b
; VBITS_GE_256-NEXT:    uzp1 z6.b, z6.b, z6.b
; VBITS_GE_256-NEXT:    uzp1 z7.b, z7.b, z7.b
; VBITS_GE_256-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_GE_256-NEXT:    ldp d13, d12, [sp, #16] // 16-byte Folded Reload
; VBITS_GE_256-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_GE_256-NEXT:    mov v5.d[1], v4.d[0]
; VBITS_GE_256-NEXT:    mov v7.d[1], v6.d[0]
; VBITS_GE_256-NEXT:    splice z3.b, p0, z3.b, z1.b
; VBITS_GE_256-NEXT:    splice z7.b, p0, z7.b, z5.b
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    st1b { z3.b }, p0, [x0, x8]
; VBITS_GE_256-NEXT:    st1b { z7.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ldr d14, [sp], #64 // 8-byte Folded Reload
; VBITS_GE_256-NEXT:    ret
;
; VBITS_EQ_512-LABEL: sabd_v64i8_v64i64:
; VBITS_EQ_512:       // %bb.0:
; VBITS_EQ_512-NEXT:    ptrue p0.d, vl8
; VBITS_EQ_512-NEXT:    mov w8, #56 // =0x38
; VBITS_EQ_512-NEXT:    mov w9, #48 // =0x30
; VBITS_EQ_512-NEXT:    mov w10, #40 // =0x28
; VBITS_EQ_512-NEXT:    mov w11, #32 // =0x20
; VBITS_EQ_512-NEXT:    mov w12, #24 // =0x18
; VBITS_EQ_512-NEXT:    mov w13, #16 // =0x10
; VBITS_EQ_512-NEXT:    mov w14, #8 // =0x8
; VBITS_EQ_512-NEXT:    ld1sb { z0.d }, p0/z, [x0, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z1.d }, p0/z, [x0, x9]
; VBITS_EQ_512-NEXT:    ld1sb { z2.d }, p0/z, [x0, x10]
; VBITS_EQ_512-NEXT:    ld1sb { z3.d }, p0/z, [x0, x11]
; VBITS_EQ_512-NEXT:    ld1sb { z4.d }, p0/z, [x0, x12]
; VBITS_EQ_512-NEXT:    ld1sb { z5.d }, p0/z, [x0, x13]
; VBITS_EQ_512-NEXT:    ld1sb { z6.d }, p0/z, [x0, x14]
; VBITS_EQ_512-NEXT:    ld1sb { z7.d }, p0/z, [x0]
; VBITS_EQ_512-NEXT:    ld1sb { z16.d }, p0/z, [x1, x8]
; VBITS_EQ_512-NEXT:    ld1sb { z17.d }, p0/z, [x1, x9]
; VBITS_EQ_512-NEXT:    ld1sb { z18.d }, p0/z, [x1, x10]
; VBITS_EQ_512-NEXT:    ld1sb { z19.d }, p0/z, [x1, x11]
; VBITS_EQ_512-NEXT:    ld1sb { z20.d }, p0/z, [x1, x12]
; VBITS_EQ_512-NEXT:    ld1sb { z21.d }, p0/z, [x1, x13]
; VBITS_EQ_512-NEXT:    ld1sb { z22.d }, p0/z, [x1, x14]
; VBITS_EQ_512-NEXT:    ld1sb { z23.d }, p0/z, [x1]
; VBITS_EQ_512-NEXT:    sabd z1.d, p0/m, z1.d, z17.d
; VBITS_EQ_512-NEXT:    sabd z0.d, p0/m, z0.d, z16.d
; VBITS_EQ_512-NEXT:    sabd z4.d, p0/m, z4.d, z20.d
; VBITS_EQ_512-NEXT:    sabd z3.d, p0/m, z3.d, z19.d
; VBITS_EQ_512-NEXT:    sabd z2.d, p0/m, z2.d, z18.d
; VBITS_EQ_512-NEXT:    sabd z6.d, p0/m, z6.d, z22.d
; VBITS_EQ_512-NEXT:    sabd z5.d, p0/m, z5.d, z21.d
; VBITS_EQ_512-NEXT:    sabd z7.d, p0/m, z7.d, z23.d
; VBITS_EQ_512-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_EQ_512-NEXT:    uzp1 z1.s, z1.s, z1.s
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl16
; VBITS_EQ_512-NEXT:    uzp1 z2.s, z2.s, z2.s
; VBITS_EQ_512-NEXT:    uzp1 z3.s, z3.s, z3.s
; VBITS_EQ_512-NEXT:    uzp1 z4.s, z4.s, z4.s
; VBITS_EQ_512-NEXT:    uzp1 z5.s, z5.s, z5.s
; VBITS_EQ_512-NEXT:    uzp1 z6.s, z6.s, z6.s
; VBITS_EQ_512-NEXT:    uzp1 z7.s, z7.s, z7.s
; VBITS_EQ_512-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_EQ_512-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_EQ_512-NEXT:    uzp1 z2.h, z2.h, z2.h
; VBITS_EQ_512-NEXT:    uzp1 z3.h, z3.h, z3.h
; VBITS_EQ_512-NEXT:    uzp1 z4.h, z4.h, z4.h
; VBITS_EQ_512-NEXT:    uzp1 z5.h, z5.h, z5.h
; VBITS_EQ_512-NEXT:    uzp1 z6.h, z6.h, z6.h
; VBITS_EQ_512-NEXT:    uzp1 z7.h, z7.h, z7.h
; VBITS_EQ_512-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_EQ_512-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_EQ_512-NEXT:    uzp1 z2.b, z2.b, z2.b
; VBITS_EQ_512-NEXT:    uzp1 z3.b, z3.b, z3.b
; VBITS_EQ_512-NEXT:    uzp1 z4.b, z4.b, z4.b
; VBITS_EQ_512-NEXT:    uzp1 z5.b, z5.b, z5.b
; VBITS_EQ_512-NEXT:    uzp1 z6.b, z6.b, z6.b
; VBITS_EQ_512-NEXT:    uzp1 z7.b, z7.b, z7.b
; VBITS_EQ_512-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_EQ_512-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_EQ_512-NEXT:    mov v5.d[1], v4.d[0]
; VBITS_EQ_512-NEXT:    mov v7.d[1], v6.d[0]
; VBITS_EQ_512-NEXT:    splice z3.b, p0, z3.b, z1.b
; VBITS_EQ_512-NEXT:    splice z7.b, p0, z7.b, z5.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl32
; VBITS_EQ_512-NEXT:    splice z7.b, p0, z7.b, z3.b
; VBITS_EQ_512-NEXT:    ptrue p0.b, vl64
; VBITS_EQ_512-NEXT:    st1b { z7.b }, p0, [x0]
; VBITS_EQ_512-NEXT:    ret
;
; VBITS_GE_2048-LABEL: sabd_v64i8_v64i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.d, vl32
; VBITS_GE_2048-NEXT:    mov w8, #32 // =0x20
; VBITS_GE_2048-NEXT:    ld1sb { z0.d }, p0/z, [x0, x8]
; VBITS_GE_2048-NEXT:    ld1sb { z1.d }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1sb { z2.d }, p0/z, [x1, x8]
; VBITS_GE_2048-NEXT:    ld1sb { z3.d }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    sabd z0.d, p0/m, z0.d, z2.d
; VBITS_GE_2048-NEXT:    sabd z1.d, p0/m, z1.d, z3.d
; VBITS_GE_2048-NEXT:    ptrue p0.b, vl32
; VBITS_GE_2048-NEXT:    uzp1 z0.s, z0.s, z0.s
; VBITS_GE_2048-NEXT:    uzp1 z1.s, z1.s, z1.s
; VBITS_GE_2048-NEXT:    uzp1 z0.h, z0.h, z0.h
; VBITS_GE_2048-NEXT:    uzp1 z1.h, z1.h, z1.h
; VBITS_GE_2048-NEXT:    uzp1 z0.b, z0.b, z0.b
; VBITS_GE_2048-NEXT:    uzp1 z1.b, z1.b, z1.b
; VBITS_GE_2048-NEXT:    splice z1.b, p0, z1.b, z0.b
; VBITS_GE_2048-NEXT:    ptrue p0.b, vl64
; VBITS_GE_2048-NEXT:    st1b { z1.b }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %a.ld = load <64 x i8>, ptr %a
  %b.ld = load <64 x i8>, ptr %b
  %a.sext = sext <64 x i8> %a.ld to <64 x i64>
  %b.sext = sext <64 x i8> %b.ld to <64 x i64>
  %sub = sub <64 x i64> %a.sext, %b.sext
  %abs = call <64 x i64> @llvm.abs.v64i64(<64 x i64> %sub, i1 true)
  %trunc = trunc <64 x i64> %abs to <64 x i8>
  store <64 x i8> %trunc, ptr %a
  ret void
}

attributes #0 = { "target-features"="+neon,+sve" }
