; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avxvnni | FileCheck %s --check-prefixes=AVXVNNI
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512vnni | FileCheck %s --check-prefixes=AVX512,AVX512VNNI
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512vnni -mattr=+avx512vl | FileCheck %s --check-prefixes=AVX512,AVX512VLVNNI

define i32 @no_dpbusd(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: no_dpbusd:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmaddwd %ymm0, %ymm2, %ymm0
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmaddwd %ymm1, %ymm2, %ymm1
; AVXVNNI-NEXT:    vphaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpermq {{.*#+}} ymm0 = ymm0[0,2,1,3]
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: no_dpbusd:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmulld %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512-NEXT:    vphaddd %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    vpermq {{.*#+}} ymm0 = ymm0[0,2,1,3]
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <16 x i8>, ptr %a, align 16
  %1 = zext <16 x i8> %0 to <16 x i32>
  %2 = load <16 x i8>, ptr %b, align 16
  %3 = zext <16 x i8> %2 to <16 x i32>
  %4 = mul nsw <16 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

define i32 @vpdpbusd_mutate(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: vpdpbusd_mutate:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovsxbd 8(%rdi), %ymm0
; AVXVNNI-NEXT:    vpmovsxbd (%rdi), %ymm1
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmaddwd %ymm0, %ymm2, %ymm0
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmaddwd %ymm1, %ymm2, %ymm1
; AVXVNNI-NEXT:    vpaddd %ymm0, %ymm1, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: vpdpbusd_mutate:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovsxbd (%rdi), %zmm0
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmulld %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <16 x i8>, ptr %a, align 16
  %1 = sext <16 x i8> %0 to <16 x i32>
  %2 = load <16 x i8>, ptr %b, align 16
  %3 = zext <16 x i8> %2 to <16 x i32>
  %4 = mul nsw <16 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

define i32 @mul_zext(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: mul_zext:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbw {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero
; AVXVNNI-NEXT:    vpmovsxbw (%rsi), %ymm1
; AVXVNNI-NEXT:    vpmullw %ymm0, %ymm1, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; AVXVNNI-NEXT:    vpmovzxwd {{.*#+}} ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: mul_zext:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbw {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero
; AVX512-NEXT:    vpmovsxbw (%rsi), %ymm1
; AVX512-NEXT:    vpmullw %ymm0, %ymm1, %ymm0
; AVX512-NEXT:    vpmovzxwd {{.*#+}} zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero,ymm0[8],zero,ymm0[9],zero,ymm0[10],zero,ymm0[11],zero,ymm0[12],zero,ymm0[13],zero,ymm0[14],zero,ymm0[15],zero
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <16 x i8>, ptr %a, align 16
  %1 = zext <16 x i8> %0 to <16 x i16>
  %2 = load <16 x i8>, ptr %b, align 16
  %3 = sext <16 x i8> %2 to <16 x i16>
  %4 = mul nsw <16 x i16> %3, %1
  ; We can't combine to vpdpbusd for zext, because each of the 4 multiplies
  ; done by vpdpbusd compute a signed 16-bit product that will be sign extended
  ; before adding into the accumulator.
  %5 = zext <16 x i16> %4 to <16 x i32>
  %6 = call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> %5)
  %op.extra = add nsw i32 %6, %c
  ret i32 %op.extra
}

define i32 @mul_sext(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: mul_sext:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbw {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero
; AVXVNNI-NEXT:    vpmovsxbw (%rsi), %ymm1
; AVXVNNI-NEXT:    vpmullw %ymm0, %ymm1, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpmovsxwd %xmm1, %ymm1
; AVXVNNI-NEXT:    vpmovsxwd %xmm0, %ymm0
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: mul_sext:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbw {{.*#+}} ymm0 = mem[0],zero,mem[1],zero,mem[2],zero,mem[3],zero,mem[4],zero,mem[5],zero,mem[6],zero,mem[7],zero,mem[8],zero,mem[9],zero,mem[10],zero,mem[11],zero,mem[12],zero,mem[13],zero,mem[14],zero,mem[15],zero
; AVX512-NEXT:    vpmovsxbw (%rsi), %ymm1
; AVX512-NEXT:    vpmullw %ymm0, %ymm1, %ymm0
; AVX512-NEXT:    vpmovsxwd %ymm0, %zmm0
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <16 x i8>, ptr %a, align 16
  %1 = zext <16 x i8> %0 to <16 x i16>
  %2 = load <16 x i8>, ptr %b, align 16
  %3 = sext <16 x i8> %2 to <16 x i16>
  %4 = mul nsw <16 x i16> %3, %1
  ; TODO:
  ; We also need to verify that the multiply has at least 2x the number of bits
  ; of the input. We shouldn't match
  ; (sign_extend (mul (vXi9 (zext (vXi8 X))), (vXi9 (zext (vXi8 Y)))).
  %5 = sext <16 x i16> %4 to <16 x i32>
  %6 = call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> %5)
  %op.extra = add nsw i32 %6, %c
  ret i32 %op.extra
}

define i32 @vpdpbusd_512(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: vpdpbusd_512:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovsxbd 8(%rsi), %ymm2
; AVXVNNI-NEXT:    vpmaddwd %ymm0, %ymm2, %ymm0
; AVXVNNI-NEXT:    vpmovsxbd (%rsi), %ymm2
; AVXVNNI-NEXT:    vpmaddwd %ymm1, %ymm2, %ymm1
; AVXVNNI-NEXT:    vpaddd %ymm0, %ymm1, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: vpdpbusd_512:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovsxbd (%rsi), %zmm1
; AVX512-NEXT:    vpmulld %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <16 x i8>, ptr %a, align 16
  %1 = zext <16 x i8> %0 to <16 x i32>
  %2 = load <16 x i8>, ptr %b, align 16
  %3 = sext <16 x i8> %2 to <16 x i32>
  %4 = mul nsw <16 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

declare i32 @llvm.vector.reduce.add.v16i32(<16 x i32>)

define i32 @vpdpbusd_256(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: vpdpbusd_256:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovsxbd (%rsi), %ymm1
; AVXVNNI-NEXT:    vpmaddwd %ymm0, %ymm1, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: vpdpbusd_256:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbd {{.*#+}} ymm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVX512-NEXT:    vpmovsxbd (%rsi), %ymm1
; AVX512-NEXT:    vpmaddwd %ymm0, %ymm1, %ymm0
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <8 x i8>, ptr %a, align 8
  %1 = zext <8 x i8> %0 to <8 x i32>
  %2 = load <8 x i8>, ptr %b, align 8
  %3 = sext <8 x i8> %2 to <8 x i32>
  %4 = mul nsw <8 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

declare i32 @llvm.vector.reduce.add.v8i32(<8 x i32>)

define i32 @vpdpbusd_128(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: vpdpbusd_128:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} xmm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero
; AVXVNNI-NEXT:    vpmovsxbd (%rsi), %xmm1
; AVXVNNI-NEXT:    vpmaddwd %xmm0, %xmm1, %xmm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: vpdpbusd_128:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbd {{.*#+}} xmm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero
; AVX512-NEXT:    vpmovsxbd (%rsi), %xmm1
; AVX512-NEXT:    vpmaddwd %xmm0, %xmm1, %xmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    retq
entry:
  %0 = load <4 x i8>, ptr %a, align 8
  %1 = zext <4 x i8> %0 to <4 x i32>
  %2 = load <4 x i8>, ptr %b, align 8
  %3 = sext <4 x i8> %2 to <4 x i32>
  %4 = mul nsw <4 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

declare i32 @llvm.vector.reduce.add.v4i32(<4 x i32>)

define i32 @vpdpbusd_2xi32(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: vpdpbusd_2xi32:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vmovq {{.*#+}} xmm0 = mem[0],zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} xmm0 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero
; AVXVNNI-NEXT:    vmovq {{.*#+}} xmm1 = mem[0],zero
; AVXVNNI-NEXT:    vpmovsxbd %xmm1, %xmm1
; AVXVNNI-NEXT:    vpmaddwd %xmm0, %xmm1, %xmm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: vpdpbusd_2xi32:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vmovq {{.*#+}} xmm0 = mem[0],zero
; AVX512-NEXT:    vpmovzxbd {{.*#+}} xmm0 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero
; AVX512-NEXT:    vmovq {{.*#+}} xmm1 = mem[0],zero
; AVX512-NEXT:    vpmovsxbd %xmm1, %xmm1
; AVX512-NEXT:    vpmaddwd %xmm0, %xmm1, %xmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    retq
entry:
  %0 = load <2 x i8>, ptr %a, align 8
  %1 = zext <2 x i8> %0 to <2 x i32>
  %2 = load <2 x i8>, ptr %b, align 8
  %3 = sext <2 x i8> %2 to <2 x i32>
  %4 = mul nsw <2 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v2i32(<2 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

declare i32 @llvm.vector.reduce.add.v2i32(<2 x i32>)

define i32 @vpdpbusd_32xi32(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: vpdpbusd_32xi32:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm3 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovsxbd 16(%rsi), %ymm4
; AVXVNNI-NEXT:    vpmaddwd %ymm0, %ymm4, %ymm0
; AVXVNNI-NEXT:    vpmovsxbd 24(%rsi), %ymm4
; AVXVNNI-NEXT:    vpmaddwd %ymm1, %ymm4, %ymm1
; AVXVNNI-NEXT:    vpmovsxbd 8(%rsi), %ymm4
; AVXVNNI-NEXT:    vpmaddwd %ymm2, %ymm4, %ymm2
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm2, %ymm1
; AVXVNNI-NEXT:    vpmovsxbd (%rsi), %ymm2
; AVXVNNI-NEXT:    vpmaddwd %ymm3, %ymm2, %ymm2
; AVXVNNI-NEXT:    vpaddd %ymm0, %ymm2, %ymm0
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: vpdpbusd_32xi32:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovsxbd (%rsi), %zmm2
; AVX512-NEXT:    vpmulld %zmm0, %zmm2, %zmm0
; AVX512-NEXT:    vpmovsxbd 16(%rsi), %zmm2
; AVX512-NEXT:    vpmulld %zmm1, %zmm2, %zmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <32 x i8>, ptr %a, align 16
  %1 = zext <32 x i8> %0 to <32 x i32>
  %2 = load <32 x i8>, ptr %b, align 16
  %3 = sext <32 x i8> %2 to <32 x i32>
  %4 = mul nsw <32 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v32i32(<32 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

declare i32 @llvm.vector.reduce.add.v32i32(<32 x i32>)

define i32 @vpdpbusd_64xi32(ptr%a, ptr%b, i32 %c, i32 %n) {
; AVXVNNI-LABEL: vpdpbusd_64xi32:
; AVXVNNI:       # %bb.0: # %entry
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm3 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm4 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm5 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm6 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovzxbd {{.*#+}} ymm7 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero
; AVXVNNI-NEXT:    vpmovsxbd 40(%rsi), %ymm8
; AVXVNNI-NEXT:    vpmaddwd %ymm0, %ymm8, %ymm0
; AVXVNNI-NEXT:    vpmovsxbd 56(%rsi), %ymm8
; AVXVNNI-NEXT:    vpmaddwd %ymm1, %ymm8, %ymm1
; AVXVNNI-NEXT:    vpmovsxbd 32(%rsi), %ymm8
; AVXVNNI-NEXT:    vpmaddwd %ymm2, %ymm8, %ymm2
; AVXVNNI-NEXT:    vpmovsxbd 48(%rsi), %ymm8
; AVXVNNI-NEXT:    vpmaddwd %ymm3, %ymm8, %ymm3
; AVXVNNI-NEXT:    vpmovsxbd 16(%rsi), %ymm8
; AVXVNNI-NEXT:    vpmaddwd %ymm4, %ymm8, %ymm4
; AVXVNNI-NEXT:    vpaddd %ymm3, %ymm4, %ymm3
; AVXVNNI-NEXT:    vpmovsxbd (%rsi), %ymm4
; AVXVNNI-NEXT:    vpmaddwd %ymm5, %ymm4, %ymm4
; AVXVNNI-NEXT:    vpaddd %ymm2, %ymm4, %ymm2
; AVXVNNI-NEXT:    vpaddd %ymm3, %ymm2, %ymm2
; AVXVNNI-NEXT:    vpmovsxbd 24(%rsi), %ymm3
; AVXVNNI-NEXT:    vpmaddwd %ymm6, %ymm3, %ymm3
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm3, %ymm1
; AVXVNNI-NEXT:    vpmovsxbd 8(%rsi), %ymm3
; AVXVNNI-NEXT:    vpmaddwd %ymm7, %ymm3, %ymm3
; AVXVNNI-NEXT:    vpaddd %ymm0, %ymm3, %ymm0
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpaddd %ymm0, %ymm2, %ymm0
; AVXVNNI-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVXVNNI-NEXT:    vpaddd %ymm1, %ymm0, %ymm0
; AVXVNNI-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVXVNNI-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVXVNNI-NEXT:    vmovd %xmm0, %eax
; AVXVNNI-NEXT:    addl %edx, %eax
; AVXVNNI-NEXT:    vzeroupper
; AVXVNNI-NEXT:    retq
;
; AVX512-LABEL: vpdpbusd_64xi32:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm0 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm1 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm2 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovzxbd {{.*#+}} zmm3 = mem[0],zero,zero,zero,mem[1],zero,zero,zero,mem[2],zero,zero,zero,mem[3],zero,zero,zero,mem[4],zero,zero,zero,mem[5],zero,zero,zero,mem[6],zero,zero,zero,mem[7],zero,zero,zero,mem[8],zero,zero,zero,mem[9],zero,zero,zero,mem[10],zero,zero,zero,mem[11],zero,zero,zero,mem[12],zero,zero,zero,mem[13],zero,zero,zero,mem[14],zero,zero,zero,mem[15],zero,zero,zero
; AVX512-NEXT:    vpmovsxbd 16(%rsi), %zmm4
; AVX512-NEXT:    vpmulld %zmm0, %zmm4, %zmm0
; AVX512-NEXT:    vpmovsxbd 48(%rsi), %zmm4
; AVX512-NEXT:    vpmulld %zmm1, %zmm4, %zmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpmovsxbd (%rsi), %zmm1
; AVX512-NEXT:    vpmulld %zmm2, %zmm1, %zmm1
; AVX512-NEXT:    vpmovsxbd 32(%rsi), %zmm2
; AVX512-NEXT:    vpmulld %zmm3, %zmm2, %zmm2
; AVX512-NEXT:    vpaddd %zmm2, %zmm1, %zmm1
; AVX512-NEXT:    vpaddd %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm1
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; AVX512-NEXT:    vpaddd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpaddd %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovd %xmm0, %eax
; AVX512-NEXT:    addl %edx, %eax
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
entry:
  %0 = load <64 x i8>, ptr %a, align 16
  %1 = zext <64 x i8> %0 to <64 x i32>
  %2 = load <64 x i8>, ptr %b, align 16
  %3 = sext <64 x i8> %2 to <64 x i32>
  %4 = mul nsw <64 x i32> %3, %1
  %5 = call i32 @llvm.vector.reduce.add.v64i32(<64 x i32> %4)
  %op.extra = add nsw i32 %5, %c
  ret i32 %op.extra
}

declare i32 @llvm.vector.reduce.add.v64i32(<64 x i32>)
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; AVX512VLVNNI: {{.*}}
; AVX512VNNI: {{.*}}
